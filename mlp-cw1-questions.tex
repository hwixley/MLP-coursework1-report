%% REPLACE sXXXXXXX with your student number
\def\studentNumber{s1803764}


%% START of YOUR ANSWERS
%% Add answers to the questions below, by replacing the text inside the brackets {} for \youranswer{ "Text to be replaced with your answer." }. 
%
% Do not delete the commands for adding figures and tables. Instead fill in the missing values with your experiment results, and replace the images with your own respective figures.
%
% You can generally delete the placeholder text, such as for example the text "Question Figure 2 - Replace the images ..." 
%
% There are 19 TEXT QUESTIONS (a few of the short first ones have their answers added to both the Introduction and the Abstract). Replace the text inside the brackets of the command \youranswer with your answer to the question.
%
% There are also 3 "questions" to replace some placeholder FIGURES with your own, and 3 "questions" asking you to fill in the missing entries in the TABLES provided. 
%
% NOTE! that questions are ordered by the order of appearance of their answers in the text, and not by the order you should tackle them. Specifically, you cannot answer Questions 2, 3, and 4 before concluding all of the relevant experiments and analysis. Similarly, you should fill in the TABLES and FIGURES before discussing the results presented there. 
%
% NOTE! If for some reason you do not manage to produce results for some FIGURES and TABLES, then you can get partial marks by discussing your expectations of the results in the relevant TEXT QUESTIONS (for example Question 8 makes use of Table 1 and Figure 2).
%
% Please refer to the coursework specification for more details.


%% - - - - - - - - - - - - TEXT QUESTIONS - - - - - - - - - - - - 

%% Question 1: define overfitting
\newcommand{\questionOne} {
\youranswer{when a model learns the ”noise” in the training data and becomes too specific so that it does not do well when generalizing to new unseen data}
}

%% Question 2: Summarise the effect increasing width and depth of the architecture had on overfitting
\newcommand{\questionTwo} {
\youranswer{improves the validation accuracy but worsens the validation error, this indicates that our model is overfitting to "noise" in the training set and thus implies that we should stop training earlier (when the validation accuracy is at it's maximum), and apply regularisation techniques to penalise large weights and ultimately reduce the complexity of the model}
}
%helps until the number of parameters becomes too high so that each layer just memorizes the training data, and you can end up with a neural network that fails to generalize well to new unseen data

%% Question 3: Summarise what your results show you about the effect of the tested approaches on overfitting and the performance of the trained model
\newcommand{\questionThree} {
\youranswer{%dropout was the best technique as it produced the best performing model with 84.45\% validation accuracy at a probability of 0.9. Out of the weight penalty techniques L1 regularisation produced the best model with 84.22\% validation accuracy using a coefficient of 1e-4.
dropout is the most effective regularisation technique for mitigating overfitting as it does a bit more than just regularisation and actually adds robustness to the network. This is due to the usage of deactivated neurons (which are ignored in the forward and backward propagations) which effectively compares our network with various different ones (given that deactivating neurons acts as a way to randomly change the dimensions of each layer) and chooses the best one. Although, the L1 and L2 regularisation techniques did not produce a validation accuracy as good as that of dropout, they did produce the best generalisation gaps and came very close to the performance produced by dropout
%as it forces nodes within a layer to probabilistically take on more or less responsibility for the inputs.s, for all combinations of hidden units and layers dropout always improved performance.
} 
}

%% Question 4: Give your overall conclusions
\newcommand{\questionFour} {
\youranswer{Q4}
}

%% Question 5: Explain what overfitting is in detail and in your own words
\newcommand{\questionFive} {
\youranswer{it is trained for too long resulting in an increasingly large generalization gap (the difference between the training error and validation error) and a decreasing validation accuracy. These trends indicate that the model is starting to learn "noise" in the training data and thus hinders the model's ability to generalize well to new unseen data such as the validation set}
}

%% Question 6: Discuss ``why'' and ``how'' overfitting occurs, and ``how'' one can identify it is happening
\newcommand{\questionSix} {
\youranswer{Overfitting occurs due to the mathematical techniques we use to train and optimise our ML models. In order to optimise our model we define an error function that represents how far our model's predictions are away from the real labels, thus to create a model with perfect accuracy we want a model that produces 0 error. So in order to produce a model that minimizes error, we can use calculus to try find a local/global minimum on this error function. However, the problem here is that this error function is defined by the training data which inevitably stores some "noise" due to various reasons (human error, data collection error, floating point round-off errors), and so as we begin to become close to 0 error we start to fit this "noise" into our model 
}
}

%% Question 7: Explain what these figures contain and how the curves evolve, and spot where overfitting occurs. Reason based on the min/max points and velocities (direction and magnitude of change) of the accuracy and error curves
\newcommand{\questionSeven} {
\youranswer{the training and validation accuracies both improve logarithmically with the number of epochs, however, when the validation accuracy reaches epoch 17 it begins to decrease linearly with the number of epochs after this. This trend indicates overfitting as after epoch 17 the validation accuracy moves further away from the training accuracy as the number of epochs increase, thus if we were given this figure when choosing the optimal model we would choose the model at epoch 18 as this gives the best validation set performance. We see that figure 1b appears like figure 1a flipped upside down, this is to be expected given these graphs are reporting the opposite metrics.}
}

%% Question 8: Explain your network width experiment results by using the relevant figure and table
\newcommand{\questionEight} {
\youranswer{the widest model (128 units) performed the best given it produced the best validation accuracy, however, it was also the most overfitted given it produced the largest generalization gap.
This seems to be the general trend from these width experiments, that as we increase the width of our model, our validation accuracy and generalization gap both increase. 
This is evident from figure~\ref{fig:width_acccurves} in which you can see that the validation accuracy of the model with 128 units peaks at epoch 20 and after this it decreases slowly and converges with the validation accuracies of the thinner models. We can see that these thinner models do not overfit as quickly as this one due to their lower complexity, this is evident in the graph as the model with 32 units continues to slowly increase in validation accuracy all the way up to epoch 94 (where it reaches it's maximum validation accuracy).
The large differences in validation accuracies seen in figure \ref{fig:depth_acccurves} at around epoch 20 are a direct indication of how increasing the width of a model can increase it's performance.
Given the results from table~\ref{tab:width_exp} alone, if I had to choose one of these models (that was trained for 100 epochs) for implementation I would choose the model with 64 units width. Although it has a slightly worse validation accuracy than that of the 128 unit model it has a far better generalization gap implying it will be a better model for generalizing to new unseen data
%the width of our model is directly proportional to validation accuracy until a certain epoch at which the model begins to become overfitted (given the validation accuracy starts to decrease and the generalization gap starts to increase). This is evident from figure 2a in which you can see that the validation accuracy of the model with 128 units peaks at epoch 20 and after this it decreases slowly and converges with the validation accuracies of the thinner models. We can see that these thinner models do not overfit as quickly as this one due to their lower complexity, this is evident in the graph as the model with 32 units continues to slowly increase in validation accuracy all the way up to epoch 94 (where it reaches it's maximum validation accuracy). From table 1 we can see that the model with 128 units is overfitted due it's near identical validation accuracy with the 64 unit model yet far larger generalization gap
}
}

%% Question 9: Discuss whether varying width affects the results in a consistent way, and whether the results are expected and match well with the prior knowledge (by which we mean your expectations as are formed from the relevant Theory and literature)
\newcommand{\questionNine} {
\youranswer{
%From these results it is evident that varying widths in this neural network architecture affected the results in a consistent way. We know that the larger the width of a model implies a larger complexity and thereby a higher susceptibility to overfitting. Thus as we increased the width of the models they became more susceptible to overfitting and so achieved their peak validation accuracies at much lower epochs. Thus table 1 is rather misleading in the quality of varying widths, as all these metrics were scored after training each model for 100 epochs rather than training each model for their optimal number of epochs.
From these results it is evident that as we increase the width of our model the validation accuracy increases, the classification error decreases, and the number of epochs needed to train our model decreases. However, we must note that this observation only holds up when we stop training our model before it becomes overfitted. We can define this point of overfitting to be the epoch when the given model reaches their optimum validation accuracy as this indicates this model is best at generalising to new unseen data. This overfitting is due to the fact that as we increase the number of training epochs our error function comes closer and closer to reaching a local minimum, but this local minimum is defined by the training samples in our dataset and thus as we get closer to this minimum our network starts to fit noise from our training data preventing it from being able to generalise well. This overfitting is evident across all our models here as they all reach their maximum validation accuracy scores before epoch 100, and after the epoch where this maximum validation accuracy is achieved we can see that for each model the validation accuracy starts to slowly decrease and the classification error starts to slowly increase.
These results were to be expected as the wider the model implies more information can be stored in the network allowing it to make more informed predictions. However, being able to store more information can also make the model more susceptible to overfitting as if it is trained for too long it can result in the network storing the "noise" from the training data and thus not allowing it to generalize well.
}
}

%% Question 10: Explain your network depth experiment results by using the relevant figure and table
\newcommand{\questionTen} {
\youranswer{the deepest model (3 layers) performed the best given it produced the best validation accuracy, however, it was also the most overfitted given it produced the largest generalization gap.
This seems to be the general trend from these depth experiments, that as we increase the depth of our model, our validation accuracy and generalization gap both increase. 
This is evident from figure~\ref{fig:depth_acccurves} in which you can see that the validation accuracy of the model with 3 layers peaks at epoch 11 and after this it decreases slowly and converges with the validation accuracies of the thinner models. We can see that these shallower models do not overfit as quickly as this one due to their lower complexity, this is evident in the graph as the model with 1 layer continues to slowly increase in validation accuracy all the way up to epoch 20 (where it reaches it's maximum validation accuracy).
Given the results from table~\ref{tab:depth_exps} alone, if I had to choose one of these models (that was trained for 100 epochs) for implementation I would choose the model with 3 layers. Although it has the worst generalization gap this model achieved the best validation accuracy performing almost 1\% better than the model with 2 layers}
}

%% Question 11: Discuss whether varying depth affects the results in a consistent way, and whether the results are expected and match well with the prior knowledge (by which we mean your expectations as are formed from the relevant Theory and literature)
\newcommand{\questionEleven} {
\youranswer{From these results it is evident that as we increase the depth of our model the validation accuracy increases, the classification error decreases, and the number of epochs needed to train our model decreases. However, we must note that this observation only holds up when we stop training our model before it becomes overfitted. We can define this point of overfitting to be the epoch when the given model reaches their optimum validation accuracy as this indicates this model is best at generalising to new unseen data. This overfitting is due to the fact that as we increase the number of training epochs our error function comes closer and closer to reaching a local minimum, but this local minimum is defined by the training samples in our dataset and thus as we get closer to this minimum our network starts to fit noise from our training data preventing it from being able to generalise well. This overfitting is evident across all our models here as they all reach their maximum validation accuracy scores before epoch 100, and after the epoch where this maximum validation accuracy is achieved we can see that for each model the validation accuracy starts to slowly decrease and the classification error starts to slowly increase.
These results were to be expected as the deeper the model implies more information can be stored in the network allowing it to make more informed predictions. However, being able to store more information can also make the model more susceptible to overfitting as if it is trained for too long it can result in the network storing the "noise" from the training data and thus not allowing it to generalize well.}
}

%% Question 12: Compare and discuss how varying width and height changes the performance and overfitting in your experiments
\newcommand{\questionTwelve} {
\youranswer{
From table~\ref{tab:width_exp} and table~\ref{tab:depth_exps} we can see that increasing depth is a more effective means to increase performance as it achieves better validation accuracy across all models (when looking at the optimal validation accuracy across 100 epochs), we can also see that increasing width makes the model less susceptible to overfitting given the far smaller generalization gaps achieved across all models.

When looking at figure \ref{fig:width_acccurves} and figure \ref{fig:depth_acccurves} we can see that there is a far larger gap in accuracies between the models of varying width which indicates the rapid improvement increasing width can have on a model's performance.

It is more computationally effective to widen the layers than increase the number of layers as wider networks allow many multiplications to be completed in parallel, unlike deeper networks which require more sequential operations (since the computations depend on the outputs of the previous layer(s)) \cite{DBLP:journals/corr/ZagoruykoK16}.
}
}

%% Question 13: Explain L1/L2 weight penalties first in words and then with formulas. Explain how they are incorporated to training and what hyperparameter(s) they require
\newcommand{\questionThirteen} {
\youranswer{
L1 and L2 weight penalties are regularisation techniques used to prevent overfitting to our training data. These techniques work by adding a regularization term to the error function which acts as a penalty for complex models with large weights. This is so that when we train our model, our error function will gravitate towards a local minimum that does not take "noisy"/insignificant features into account. The mathematical formula for the loss function without regularization is as follows:
$$ E^n = E^{n}_{train}$$

L1 regularisation is the preferred choice when having a high number of features as it provides sparse solutions. In L1 the weights shrink to 0 at a constant rate ($\beta \text{sgn}(w_i)$), thus the new mathematical formula for the loss function with L1 regularization is as follows:
$$ E^n = E^{n}_{train} + \beta \sum_{i=1}^{n}|w_i|$$
$$ \text{such that}$$

$$ \frac{\partial E^n}{\partial w_i} = \frac{\partial E_{train}^{n}}{\partial w_i} + \beta \text{sgn}(w_i)$$
Where $ \beta$ is the regularization parameter and $ \text{sgn}(w_i)$ is the sign of $w_i$. 
\\

L2 regularisation can deal with multicollinearity (independent variables that are highly correlated) problems through constricting the coefficient and by keeping all the variables. L2 regression can be used to estimate the significance of predictors and based on that it can penalize the insignificant predictors. In L2 weights shrink to 0 at a rate proportional to the size of the weight ($ \beta w_i$), thus the new mathematical formula for the loss function with L2 regression is as follows:
$$ E^n = E^{n}_{train} + \beta \sum_{i=1}^{n}w_i^2 $$
$$ \text{such that} $$

$$ \frac{\partial E^n}{\partial w_i} = \frac{\partial E_{train}^{n}}{\partial w_i} + \beta w_i$$
Where $ \beta$ is the regularization parameter.
\\

This regularization parameter $\beta$ is evidently extremely important to both these regularization techniques as it directly affects the complexity and training-data fit of the model. Given this parameter is handpicked it is extremely important that it is set optimally. The choice of this parameter is entirely dependent on how far we want to simplify our model (as increasing it's magnitude strengthens this regularization effect) so we must keep the initial performance and fitting of our model in mind when choosing this. A good method to choose this parameter would be to test varying size $ \beta$ values on the validation set and choose the one that produces the best validation accuracy
}
}

%% Question 14: Discuss how/why the weight penalties may address overfitting, discuss how L1 and L2 regularization differ and support your claims with references where possible
\newcommand{\questionFourteen} {
\youranswer{
Weight penalties address overfitting by penalising large weights in models, penalising these large weights helps to reduce the complexity of the model and ultimately retain less noise from the training set.

L1 and L2 regularisation differ in the regularisation terms they add to the error function. In L1 regression weights shrink to 0 at a constant rate in contrast to L2 regression where weights shrink to 0 at a rate proportional to the magnitude of the weight. The differences in these regularization terms imply that these techniques can be used in different contexts. L1 regularisation is particularly useful for feature selection as we can drop features with coefficients that go to zero. L2 regularisation on the other hand is useful when you have collinear/codependent features as codependence tends to increase coefficient variance, making coefficients unreliable/unstable which ends up hurting the model's generality. L2 fixes this by reducing the variance of these estimates which counteracts the effect of codependencies. 
}
}

%% Question 15: Explain the experimental details (e.g. hyperparameters), discuss the results in terms of their generalization performance and overfitting
\newcommand{\questionFifteen} {
\youranswer{Q15}
}

%% Question 16: Explain the motivation behind Maxout Networks as presented in \cite{goodfellow2013maxout}
\newcommand{\questionSixteen} {
\youranswer{Q16
}
}

%% Question 17: State whether Dropout is compatible (can be used together) with Maxout and explain why
\newcommand{\questionSeventeen} {
\youranswer{Q17
}
}

%% Question 18: Give an overview of the experiment setup in \cite{goodfellow2013maxout} and analyse it from the point of view of how convincing their conclusions are 
\newcommand{\questionEighteen} {
\youranswer{Q18
}
}

%% Question 19: Briefly draw your conclusions based on the results from the previous sections (what are the take-away messages?) and conclude your report with a recommendation for future directions
\newcommand{\questionNineteen} {
\youranswer{Q19
}
}


%% - - - - - - - - - - - - FIGURES - - - - - - - - - - - - 

%% Question Figure 2:
\newcommand{\questionFigureTwo} {
\youranswer{%Question Figure 2 - Replace the images in Figure 2 with figures depicting the accuracy and error, training and validation curves for your experiments varying the number of hidden units.
%
\begin{figure}[t]
    \centering
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/width-fig1.pdf}
        \caption{accuracy by epoch}
        \label{fig:width_acccurves}
    \end{subfigure} 
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/width-fig2.pdf}
        \caption{error by epoch}
        \label{fig:width_errorcurves}
    \end{subfigure} 
    \caption{Training and validation curves in terms of classification accuracy (a) and cross-entropy error (b) on the EMNIST dataset for different network widths.}
    \label{fig:width}
\end{figure} 
}
}

%% Question Figure 3:
\newcommand{\questionFigureThree} {
\youranswer{%Question Figure 3 - Replace these images with figures depicting the accuracy and error, training and validation curves for your experiments varying the number of hidden layers.
%
\begin{figure}[t]
    \centering
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/depth-fig1.pdf}
        \caption{accuracy by epoch}
        \label{fig:depth_acccurves}
    \end{subfigure} 
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/depth-fig2.pdf}
        \caption{error by epoch}
        \label{fig:depth_errorcurves}
    \end{subfigure} 
    \caption{Training and validation curves in terms of classification accuracy (a) and cross-entropy error (b) on the EMNIST dataset for different network depths.}
    \label{fig:depth}
\end{figure} 
}
}

%% Question Figure 4:
\newcommand{\questionFigureFour} {
\youranswer{Question Figure 4 - Replace these images with figures depicting the Validation Accuracy and Generalisation Gap for each of your experiments varying the Dropout inclusion rate, L1/L2 weight penalty, and for the 8 combined experiments (you will have to find a way to best display this information in one subfigure).
%
\begin{figure*}[t]
    \centering
    \begin{subfigure}{.3\linewidth}
        \includegraphics[width=\linewidth]{figures/dropout-val-acc-gen-gap.pdf}
        \caption{Metrics by inclusion rate}
        \label{fig:dropoutrates}
    \end{subfigure} 
    \begin{subfigure}{.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/l1-l2-val-acc-gen-gap.pdf}
        \caption{Metrics by weight penalty}
        \label{fig:weightrates}
    \end{subfigure} 
    \begin{subfigure}{.3\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/combined-val-acc-gen-gap-wmeans.pdf}
        \caption{Metrics by combining inclusion rate and weight penalty}
        \label{fig:extra}
    \end{subfigure} 
    \caption{Hyperparameter search for every method and combinations}
    \label{fig:hp_search}
\end{figure*}
}
}

%% - - - - - - - - - - - - TABLES - - - - - - - - - - - - 

%% Question Table 1:
\newcommand{\questionTableOne} {
\youranswer{
%Question Table 1 - Fill in Table 1 with the results from your experiments varying the number of hidden units.
%
\begin{table}[t]
    \centering
    \begin{tabular}{c|cc}
    \toprule
        \# hidden units & val. acc. & generalization gap \\
    \midrule
         32            &      77.94\%      &      0.148              \\
         64            &      80.91\%      &      0.344              \\
         128           &      80.92\%      &      0.803              \\ 
    \bottomrule
    \end{tabular}
    \caption{Validation accuracy (\%) and generalization gap (in terms of cross-entropy error) for varying network widths on the EMNIST dataset.}
    \label{tab:width_exp}
\end{table}
}
}

%% Question Table 2:
\newcommand{\questionTableTwo} {
\youranswer{
%Question Table 2 - Fill in Table 2 with the results from your experiments varying the number of hidden layers.
%
\begin{table}[t]
    \centering
    \begin{tabular}{c|cc}
    \toprule
        \# hidden layers & val. acc. & generalization gap \\
    \midrule
         1               &      80.92\%      &    0.803               \\
         2               &      81.56\%      &    1.456               \\
         3               &      82.51\%      &    1.538               \\ 
    \bottomrule
    \end{tabular}
    \caption{Validation accuracy (\%) and generalization gap (in terms of cross-entropy error) for varying network depths on the EMNIST dataset.}
    \label{tab:depth_exps}
\end{table}
}
}

%% Question Table 3:
\newcommand{\questionTableThree} {
\youranswer{
Question Table 3 - Fill in Table 3 with the results from your experiments varying the hyperparameter values for each of L1 regularisation, L2 regularisation, and Dropout (use the values shown on the table) as well as the results for your experiments combining L1/L2 and Dropout (you will have to pick what combinations of hyperparameter values to test for the combined experiments; each of the combined experiments will need to use Dropout and either L1 or L2 regularisation; run an experiment for each of 8 different combinations). Use \textit{italics} to print the best result per criterion for each set of experiments, and \textbf{bold} for the overall best result per criterion.
%
\begin{table*}[t]
    \centering
    \begin{tabular}{c|c|cc}
    \toprule
        Model    &  Hyperparameter value(s) & Validation accuracy & Generalization gap \\
    \midrule
    \midrule
        Baseline &  -                    &               0.836 &                 0.290 \\
    \midrule
        \multirow{3}*{Dropout}
                 & 0.7                   &   81.68\%                   &  \emph{0.030}    \\
                 & 0.9                   &   85.78\%   &  0.095           \\
                 & 0.95                  &   \emph{86.15\%}                   &  0.142           \\
    \midrule
        \multirow{3}*{L1 penalty}
                 & 1e-4                   &  \emph{84.60}\%                   &  0.078           \\
                 & 1e-3                   &  73.20\%                   &  0.008           \\
                 & 1e-1                   &  1.98\%                    &  \textbf{\emph{0}}               \\
    \midrule
        \multirow{3}*{L2 penalty}  
                 & 1e-4                   &  84.45\%                   & 0.227            \\
                 & 1e-3                   &  \emph{84.80}\%                   & 0.098            \\
                 & 1e-1                   &  38.77\%                   & \textbf{\emph{0}}                \\
    \midrule
        \multirow{6}*{Combined}  
                 & 0.95, L1 1e-5  &      86.19\%               &   0.127                  \\
                 & 0.95, L1 1e-6                    &     86.05\%                &              0.139           \\
                 & 0.9, L2 1e-4                   &    86.03\%                 &      \emph{0.078}                   \\
                 & 0.95, L2 1e-4                   &     86.16\%                &      0.120                   \\
                 & 0.95, L2 1e-5                   &   86.04\%                  &      0.145                   \\
                 & 0.95, L2 1e-6                   &  \textbf{\emph{ 86.56\%}}                  &      0.142                   \\
    \bottomrule
    \end{tabular}
    \caption{Results of all hyperparameter search experiments. \emph{italics} indicate the best results per series and \textbf{bold} indicate the best overall}
    \label{tab:hp_search}
\end{table*}
}
}

%% END of YOUR ANSWERS